{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d77fbb9-8337-458d-9d04-79ed003c94bf",
   "metadata": {},
   "source": [
    "# Stacking in Neural Networks\n",
    "Stacking is an ensemble learning technique where multiple models (often of different types) are trained and combined to improve the overall performance. In the context of neural networks, stacking involves training several neural networks and then using another model (often a neural network) to combine their predictions.\n",
    "\n",
    "\n",
    "* Base Models: \n",
    "Train multiple neural networks (or other models) on the training data. These models are called base models or level-0 models.\n",
    "* Meta-Model: \n",
    "Train a meta-model (or level-1 model) on the predictions of the base models. The meta-model learns how to best combine the predictions of the base models to improve accuracy.\n",
    "\n",
    "## Important Terminologies\n",
    "* Epoch: \n",
    "One complete pass through the entire training dataset. Neural networks are typically trained for multiple epochs.\n",
    "Batch Size: The number of training samples used in one iteration of model training. Smaller batch sizes can lead to more stable updates, while larger batch sizes can speed up training.\n",
    "Learning Rate: A hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated.\n",
    "* Activation Function:\n",
    "A function applied to the output of each neuron. Common activation functions include ReLU, Sigmoid, and Tanh.\n",
    "Loss Function: A function that measures how well the model’s predictions match the actual data. Common loss functions include Mean Squared Error (MSE) for regression and Cross-Entropy Loss for classification.\n",
    "* Optimizer:\n",
    "An algorithm used to minimize the loss function by updating the model’s weights. Common optimizers include Stochastic Gradient Descent (SGD), Adam, and RMSprop.\n",
    "## Hyperparameters in Neural Networks\n",
    "     - Number of Layers: The depth of the neural network, including input, hidden, and output layers.\n",
    "     - Number of Neurons per Layer: The number of neurons in each layer, which can affect the model’s capacity and complexity.\n",
    "     - Learning Rate: As mentioned, it controls the step size during gradient descent.\n",
    "     - Batch Size: Also mentioned, it affects the stability and speed of training.\n",
    "     - Dropout Rate: A regularization technique where a fraction of neurons is randomly turned off during training to prevent overfitting.\n",
    "     - Weight Initialization: The method used to initialize the weights of the network. Common methods include Xavier and He initialization.\n",
    "     - Activation Functions: The choice of activation function for each layer.\n",
    "     - Epochs: The number of times the entire training dataset is passed through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c1552-ab81-4284-8710-2fe5b91d9b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
